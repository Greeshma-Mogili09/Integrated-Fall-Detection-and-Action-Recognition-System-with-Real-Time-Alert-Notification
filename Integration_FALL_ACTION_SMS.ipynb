{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Action Recognition...\n",
      "Processing video: C:\\Users\\mgree\\Downloads\\F1.avi\n",
      "FPS: 20.0, Total Frames: 212, Duration: 10.60 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "Predictions: [[4.4350110e-12 1.3825411e-07 9.9999988e-01 1.4700746e-11 6.9757072e-20\n",
      "  9.2104822e-12 1.5204419e-14 4.4069060e-16 1.6993846e-14 5.6892245e-17]], Predicted Class: 2\n",
      "Frame 40: Action: Fall left\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Predictions: [[2.9197856e-14 4.4646047e-08 9.9999928e-01 2.1693753e-09 1.8539154e-18\n",
      "  6.6428493e-07 4.6437021e-12 1.4012966e-11 2.8334939e-12 1.3068129e-14]], Predicted Class: 2\n",
      "Frame 50: Action: Fall left\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Predictions: [[1.8475749e-08 4.2433576e-06 9.9999392e-01 2.1356779e-09 9.2258740e-10\n",
      "  1.7105975e-06 7.0671451e-09 3.1926334e-08 3.0696974e-09 5.2321916e-08]], Predicted Class: 2\n",
      "Frame 60: Action: Fall left\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predictions: [[7.8937759e-08 1.4619106e-07 9.9999845e-01 8.2586760e-10 6.8969804e-12\n",
      "  2.6801734e-09 6.0507110e-10 2.6446836e-08 1.5653466e-11 1.2422911e-06]], Predicted Class: 2\n",
      "Frame 70: Action: Fall left\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Predictions: [[6.8252760e-20 1.6014863e-11 1.0000000e+00 2.0744349e-11 7.6631707e-24\n",
      "  5.5979142e-13 1.6246863e-20 9.1030068e-19 1.2213891e-16 3.1252321e-20]], Predicted Class: 2\n",
      "Frame 80: Action: Fall left\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Predictions: [[2.4437512e-33 2.0617852e-22 1.0000000e+00 1.3053500e-24 0.0000000e+00\n",
      "  1.2836113e-27 5.6965266e-38 0.0000000e+00 1.1896827e-31 0.0000000e+00]], Predicted Class: 2\n",
      "Frame 90: Action: Fall left\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "Predictions: [[5.3581474e-25 1.0605264e-15 1.0000000e+00 1.6356219e-16 4.8029823e-30\n",
      "  2.7722251e-21 4.9546230e-26 1.1763409e-27 8.2283339e-20 2.9668232e-27]], Predicted Class: 2\n",
      "Frame 100: Action: Fall left\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predictions: [[2.2936492e-28 4.0609879e-19 1.0000000e+00 8.8324585e-21 1.3819242e-34\n",
      "  2.0855934e-24 7.4251930e-31 1.1376211e-33 2.2531613e-26 7.6182797e-33]], Predicted Class: 2\n",
      "Frame 110: Action: Fall left\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Predictions: [[4.9446672e-28 4.4785597e-16 1.0000000e+00 1.7338104e-19 2.5484438e-36\n",
      "  4.5506727e-21 7.7785003e-33 9.3851276e-34 7.3455068e-27 2.9534416e-35]], Predicted Class: 2\n",
      "Frame 120: Action: Fall left\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predictions: [[7.6189519e-32 5.7555544e-20 1.0000000e+00 5.0947657e-27 8.1518804e-36\n",
      "  7.5913599e-25 1.5256796e-37 4.3000954e-38 1.3505786e-31 0.0000000e+00]], Predicted Class: 2\n",
      "Frame 130: Action: Fall left\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "Predictions: [[1.1922771e-23 7.5483937e-17 1.0000000e+00 4.5186747e-19 1.5656142e-27\n",
      "  9.4871876e-24 7.1280864e-30 2.6360682e-29 1.9773286e-24 4.3467628e-33]], Predicted Class: 2\n",
      "Frame 140: Action: Fall left\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Predictions: [[1.8752724e-22 1.6728224e-13 1.0000000e+00 5.4834738e-18 1.5461771e-26\n",
      "  1.1966793e-22 1.2658958e-28 1.3080013e-25 9.8678997e-24 2.6263668e-33]], Predicted Class: 2\n",
      "Frame 150: Action: Fall left\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Predictions: [[9.9168141e-22 3.9762998e-12 1.0000000e+00 2.1630963e-17 2.1040938e-23\n",
      "  1.6814423e-18 1.0019463e-25 1.7478178e-24 6.2533597e-22 4.8146071e-32]], Predicted Class: 2\n",
      "Frame 160: Action: Fall left\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Predictions: [[6.0794423e-22 2.9437236e-12 1.0000000e+00 1.0723263e-18 3.2184871e-22\n",
      "  7.2357034e-21 3.5669677e-25 2.7740843e-23 7.7463854e-22 3.8689998e-32]], Predicted Class: 2\n",
      "Frame 170: Action: Fall left\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Predictions: [[1.2803778e-21 5.5494585e-13 1.0000000e+00 2.5376323e-17 1.4714193e-21\n",
      "  3.0393242e-19 3.0449449e-24 5.0695149e-23 1.6098491e-20 7.1554322e-32]], Predicted Class: 2\n",
      "Frame 180: Action: Fall left\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Predictions: [[1.3315098e-20 4.1010324e-13 1.0000000e+00 2.0498218e-18 3.4542313e-25\n",
      "  1.6078446e-17 7.8799821e-26 5.2875855e-26 3.5683057e-23 2.1534298e-33]], Predicted Class: 2\n",
      "Frame 190: Action: Fall left\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Predictions: [[4.2163351e-20 5.5051276e-13 1.0000000e+00 2.7457523e-16 1.3513953e-27\n",
      "  1.0367203e-19 9.0122071e-26 1.8808846e-27 1.0386228e-22 3.0801130e-29]], Predicted Class: 2\n",
      "Frame 200: Action: Fall left\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Predictions: [[4.3266411e-28 1.3992010e-19 1.0000000e+00 9.5173637e-20 1.2576901e-34\n",
      "  9.8595465e-24 1.6923457e-30 7.2349364e-35 2.4105040e-25 1.3553401e-34]], Predicted Class: 2\n",
      "Frame 210: Action: Fall left\n",
      "Actions detected: ['Fall left', 'Fall left', 'Fall left', 'Fall left', 'Fall left', 'Fall left', 'Fall left', 'Fall left', 'Fall left', 'Fall left', 'Fall left', 'Fall left', 'Fall left', 'Fall left', 'Fall left', 'Fall left', 'Fall left', 'Fall left']\n",
      "\n",
      "Starting Fall Detection...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "Fall Detection Result: Fall\n",
      "SMS Alert Sent!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from twilio.rest import Client\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import custom_object_scope\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Multiply, Add, Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "class SpatialAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv = Conv2D(1, kernel_size=7, padding='same', activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        avg_pool = tf.reduce_mean(inputs, axis=-1, keepdims=True)\n",
    "        max_pool = tf.reduce_max(inputs, axis=-1, keepdims=True)\n",
    "        \n",
    "        concat = tf.concat([avg_pool, max_pool], axis=-1)\n",
    "        \n",
    "        attention = self.conv(concat)\n",
    "        \n",
    "        return Multiply()([inputs, attention])\n",
    "\n",
    "\n",
    "class ChannelAttention(Layer):\n",
    "    def __init__(self, reduction_ratio=8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.dense1 = Dense(input_shape[-1] // self.reduction_ratio, activation='relu')\n",
    "        self.dense2 = Dense(input_shape[-1], activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        avg_pool = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n",
    "        max_pool = tf.reduce_max(inputs, axis=[1, 2], keepdims=True)\n",
    "\n",
    "        avg_out = self.dense2(self.dense1(avg_pool))\n",
    "        max_out = self.dense2(self.dense1(max_pool))\n",
    "\n",
    "        attention = Add()([avg_out, max_out])\n",
    "        return Multiply()([inputs, attention])\n",
    "\n",
    "with custom_object_scope({'SpatialAttention': SpatialAttention, 'ChannelAttention': ChannelAttention}):\n",
    "    action_recognition_model = load_model(\"alternate_attention_model_with_flattening.h5\")\n",
    "\n",
    "fall_detection_model = load_model(\"model_attention.h5\")  \n",
    "\n",
    "ACTION_CATEGORIES = {\n",
    "    0: 'Fall forward',\n",
    "    1: 'Fall backwards',\n",
    "    2: 'Fall left',\n",
    "    3: 'Fall right',\n",
    "    4: 'Fall sitting',\n",
    "    5: 'Walk',\n",
    "    6: 'Hop',\n",
    "    7: 'Pick up object',\n",
    "    8: 'Sit down',\n",
    "    9: 'Kneel'\n",
    "}\n",
    "\n",
    "\n",
    "ACCOUNT_SID = 'XXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "AUTH_TOKEN = XXXXXXXXXXXXXXXXXXXXXXXXXXXc'\n",
    "TWILIO_PHONE = '+XXXXXXXXXX'\n",
    "RECIPIENT_PHONE = '+XXXXXXXXXXXXX'\n",
    "\n",
    "client = Client(ACCOUNT_SID, AUTH_TOKEN)\n",
    "\n",
    "def open_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Cannot open video file: {video_path}\")\n",
    "    return cap\n",
    "\n",
    "def send_sms_alert(message, to_phone):\n",
    "    try:\n",
    "        client.messages.create(\n",
    "            body=message,\n",
    "            from_=TWILIO_PHONE,\n",
    "            to=to_phone\n",
    "        )\n",
    "        print(\"SMS Alert Sent!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to send SMS: {e}\")\n",
    "\n",
    "def predict_action(frame, model):\n",
    "    try:\n",
    "        frame_resized = cv2.resize(frame, (112, 112))\n",
    "        frame_normalized = frame_resized / 255.0\n",
    "        frame_batch = np.expand_dims(frame_normalized, axis=0)\n",
    "\n",
    "        predictions = model.predict(frame_batch)\n",
    "        predicted_class = np.argmax(predictions)\n",
    "        print(f\"Predictions: {predictions}, Predicted Class: {predicted_class}\")\n",
    "        predicted_action = ACTION_CATEGORIES.get(predicted_class, \"Unknown Action\")\n",
    "        return predicted_action, predictions[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting action: {e}\")\n",
    "        return \"Error\", None\n",
    "\n",
    "\n",
    "\n",
    "def predict_fall(video_path, model, batch_size=4):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames_batch = []\n",
    "    fall_predictions = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_resized = cv2.resize(frame, (224, 224))  \n",
    "        frames_batch.append(frame_resized)\n",
    "\n",
    "        if len(frames_batch) == batch_size:\n",
    "            batch_array = np.array(frames_batch)\n",
    "            predictions = model.predict(batch_array)\n",
    "            fall_predictions.extend(np.argmax(predictions, axis=1))\n",
    "            frames_batch = []\n",
    "\n",
    "    if len(frames_batch) > 0: \n",
    "        batch_array = np.array(frames_batch)\n",
    "        predictions = model.predict(batch_array)\n",
    "        fall_predictions.extend(np.argmax(predictions, axis=1))\n",
    "\n",
    "    cap.release()\n",
    "   \n",
    "    fall_count = fall_predictions.count(1)\n",
    "    non_fall_count = fall_predictions.count(0)\n",
    "    final_prediction = \"Fall\" if fall_count > non_fall_count else \"Non-fall\"\n",
    "    return final_prediction\n",
    "\n",
    "def process_video_for_action(video_path, model, start_frame=40, frame_skip=10):\n",
    "    cap = open_video(video_path)  \n",
    "    frame_count = 0\n",
    "    actions = []\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    total_duration = total_frames / fps  \n",
    "\n",
    "    print(f\"Processing video: {video_path}\")\n",
    "    print(f\"FPS: {fps}, Total Frames: {total_frames}, Duration: {total_duration:.2f} seconds\")\n",
    "\n",
    "    output_path = \"output_with_action_labels.avi\"\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  \n",
    "\n",
    "        if frame_count >= start_frame and frame_count % frame_skip == 0:  # Start predicting after `start_frame`\n",
    "            predicted_action, _ = predict_action(frame, model) \n",
    "            actions.append(predicted_action) \n",
    "\n",
    "            label = f\"Action: {predicted_action}\"\n",
    "            print(f\"Frame {frame_count}: {label}\") \n",
    "\n",
    "            text_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "            text_width, text_height = text_size\n",
    "            cv2.rectangle(frame, (10, 10), (10 + text_width, 30 + text_height), (0, 0, 0), -1)  \n",
    "            cv2.putText(frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2) \n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        cv2.imshow(\"Action Recognition\", frame)\n",
    "\n",
    "        if cv2.waitKey(300) & 0xFF == ord('q'): \n",
    "            print(\"Video processing interrupted by user.\")\n",
    "            break\n",
    "\n",
    "        if frame_count > (fps * 300): \n",
    "            print(\"Stopping after 50 seconds of playback.\")\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release() \n",
    "    out.release()  \n",
    "    cv2.destroyAllWindows()  \n",
    "    return actions\n",
    "\n",
    "\n",
    "def process_video_pipeline(video_path):\n",
    "    try:\n",
    "        print(\"Starting Action Recognition...\")\n",
    "        actions = process_video_for_action(video_path, action_recognition_model)\n",
    "        print(f\"Actions detected: {actions}\")\n",
    "\n",
    "        print(\"\\nStarting Fall Detection...\")\n",
    "        fall_prediction = predict_fall(video_path, fall_detection_model)\n",
    "        print(f\"Fall Detection Result: {fall_prediction}\")\n",
    "\n",
    "        if fall_prediction == \"Fall\":\n",
    "            alert_message = f\"Alert! A fall has been detected \"\n",
    "            send_sms_alert(alert_message, RECIPIENT_PHONE)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in processing pipeline: {e}\")\n",
    "\n",
    "video_path = r\"C:\\Users\\mgree\\Downloads\\F1.avi\"\n",
    "process_video_pipeline(video_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
